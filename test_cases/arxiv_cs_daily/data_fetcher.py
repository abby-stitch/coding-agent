# data_fetcher.py â€”â€” çœŸå® arXiv æ•°æ®æŠ“å–å™¨ï¼ˆData Agentï¼‰
import json
import urllib.request
from datetime import datetime
from xml.etree import ElementTree as ET
from pathlib import Path

CATEGORIES = ["cs.AI", "cs.TH", "cs.CV", "cs.LG"]
MAX_RESULTS = 5  # æ¯ç±»æœ€å¤š 5 ç¯‡

def fetch_arxiv_papers():
    all_papers = []
    for cat in CATEGORIES:
        print(f"  â†’ è·å– {cat} çš„æœ€æ–°è®ºæ–‡...")
        query = f"cat:{cat}"
        url = f"http://export.arxiv.org/api/query?search_query={query}&max_results={MAX_RESULTS}&sortBy=submittedDate&sortOrder=descending"

        try:
            with urllib.request.urlopen(url) as response:
                xml_data = response.read()
            papers = parse_arxiv_response(xml_data, cat)
            all_papers.extend(papers)
            print(f"    âœ… æ‰¾åˆ° {len(papers)} ç¯‡")
        except Exception as e:
            print(f"    âš ï¸ å¤±è´¥: {e}")
    
    return all_papers

def parse_arxiv_response(xml_data, expected_category):
    ns = {'atom': 'http://www.w3.org/2005/Atom'}
    root = ET.fromstring(xml_data)
    papers = []

    for entry in root.findall('atom:entry', ns):
        title = entry.find('atom:title', ns).text.strip().replace('\n', ' ')
        authors = [a.find('atom:name', ns).text for a in entry.findall('atom:author', ns)]
        published = entry.find('atom:published', ns).text.split('T')[0]
        arxiv_id = entry.find('atom:id', ns).text.split('/')[-1]

        pdf_url = f"https://arxiv.org/pdf/{arxiv_id}"

        papers.append({
            "id": arxiv_id,
            "title": title,
            "authors": authors,
            "category": expected_category,
            "submit_date": published,
            "pdf_url": pdf_url
        })
    return papers

def run_data_agent():
    """
    Data Agent ä¸»å…¥å£ï¼šä» arXiv æŠ“å–çœŸå®è®ºæ–‡æ•°æ®ï¼Œè¾“å‡ºä¸º outputs/papers.jsonã€‚
    ä¸ä½¿ç”¨ LLMï¼Œç¡®ä¿æ•°æ®çœŸå®æ€§ã€‚
    """
    print("  â†’ å¼€å§‹ä» arXiv æŠ“å–çœŸå®è®ºæ–‡æ•°æ®...")
    papers = fetch_arxiv_papers()

    if not papers:
        print("    âš ï¸ æœªè·å–åˆ°ä»»ä½•è®ºæ–‡ï¼Œç”Ÿæˆå°‘é‡ç¤ºä¾‹æ•°æ®ç”¨äºæµ‹è¯•")
        papers = [{
            "id": "2412.99999",
            "title": "Sample Paper for Testing",
            "authors": ["Test Author"],
            "category": "cs.AI",
            "submit_date": datetime.utcnow().strftime('%Y-%m-%d'),
            "pdf_url": "https://arxiv.org/pdf/2412.99999"
        }]

    # âœ… ä¿®æ­£ï¼šä»è„šæœ¬ä½ç½®å‘ä¸Šä¸¤çº§åˆ°è¾¾é¡¹ç›®æ ¹ç›®å½•
    script_dir = Path(__file__).resolve().parent      # .../test_cases/arxiv_cs_daily
    project_root = script_dir.parent.parent           # .../test_cases â†’ .../project-root
    output_dir = project_root / "outputs"
    output_dir.mkdir(exist_ok=True)
    output_path = output_dir / "data.json"

    # å†™å…¥æ–‡ä»¶
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(papers, f, ensure_ascii=False, indent=2)

    print(f"  ğŸ‰ å…±ä¿å­˜ {len(papers)} ç¯‡è®ºæ–‡åˆ° {output_path.resolve()}")

# âœ… è„šæœ¬å…¥å£ï¼šå¿…é¡»é¡¶æ ¼ï¼
if __name__ == "__main__":
    run_data_agent()