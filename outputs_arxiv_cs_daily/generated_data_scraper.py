# -*- coding: utf-8 -*-
# This script was auto-generated by Data Agent based on data_prompt.txt
import feedparser
import json
import re

def parse_arxiv_rss(category):
    url = f"http://export.arxiv.org/rss/{category}"
    feed = feedparser.parse(url)
    papers = []
    for entry in feed.entries:
        id_match = re.search(r'(\d{4}\.\d{5})', entry.id)
        if not id_match:
            continue
        paper_id = id_match.group(1)
        title = entry.title.strip()
        authors = [author.name for author in entry.authors]
        category_label = category
        submit_date = entry.published_parsed[:3]
        pdf_url = f"https://arxiv.org/pdf/{paper_id}.pdf"
        papers.append({
            "id": paper_id,
            "title": title,
            "authors": authors,
            "category": category_label,
            "submit_date": "-".join(map(str, submit_date)),
            "pdf_url": pdf_url
        })
        if len(papers) >= 2:
            break
    return papers

def main():
    categories = ["cs.AI", "cs.TH", "cs.CV", "cs.LG"]
    all_papers = []
    for category in categories:
        try:
            papers = parse_arxiv_rss(category)
            all_papers.extend(papers)
        except Exception as e:
            print(f"Error processing {category}: {e}")
    print(json.dumps(all_papers, indent=2))

if __name__ == "__main__":
    main()